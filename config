[NN]
dim_val = 512 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.
n_heads = 8 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number
n_decoder_layers = 4 # Number of times the decoder layer is stacked in the decoder
n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder
input_size = 2 # The number of input variables. 1 if univariate forecasting.
dec_seq_len = 168 # length of input given to decoder. Can have any integer value.
enc_seq_len = 168 # length of input given to encoder. Can have any integer value. # supposing you want the model to base its forecasts on the previous 7 days of data = 168
output_sequence_length = 1 # Length of the target sequence, i.e. how many time steps should your forecast cover # supposing you're forecasting 48 hours ahead = 48
window_size = enc_seq_len + output_sequence_length # used to slice data into sub-sequences
step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step
max_seq_len = enc_seq_len # What's the longest sequence the model will encounter? Used to make the positional encoder
batch_first = True
batch_size = 20
training_set_size = 5000
valid_set_size = 500
test_set_size = 500
epochs = 1000


[restful]
url=http://localhost:5000
service_port=5000

[mogodb]
database_name: curixCoreDb 
dbHost: 10.103.10.120
dbPort: 7000
dbUsername: curix
dbPassword: persistence2022
